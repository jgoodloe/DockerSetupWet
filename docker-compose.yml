services:
  # =======================================================
  # 1. OPEN-APPSEC STACK (Replaces Standard NPM)
  # =======================================================
  
  # The Agent that communicates with the SaaS Cloud
  appsec-agent:
    image: ghcr.io/openappsec/agent:${APPSEC_VERSION}
    container_name: appsec-agent
    ipc: host # CRITICAL: Must be 'host' for shared memory with NPM
    restart: unless-stopped
    labels:
      - "com.centurylinklabs.watchtower.enable=false" # Critical service - manual updates only
    environment:
      - https_proxy=${APPSEC_HTTPS_PROXY}
      - user_email=${APPSEC_USER_EMAIL}
      - AGENT_TOKEN=${APPSEC_AGENT_TOKEN}
      - autoPolicyLoad=${APPSEC_AUTO_POLICY_LOAD}
      - nginxproxymanager=true
    volumes:
      - ${APPSEC_CONFIG}:/etc/cp/conf
      - ${APPSEC_DATA}:/etc/cp/data
      - ${APPSEC_LOGS}:/var/log/nano_agent
      - ${APPSEC_LOCALCONFIG}:/ext/appsec
    command: /cp-nano-agent
    networks:
      - npm_network
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "[ -f /etc/cp/conf/ready ] || pgrep -f cp-nano-agent || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # The NGINX Proxy Manager with open-appsec attachment
  appsec-nginx-proxy-manager:
    container_name: appsec-nginx-proxy-manager
    # Official image for Central (SaaS) Management
    image: ghcr.io/openappsec/nginx-proxy-manager-centrally-managed-attachment:${APPSEC_VERSION}
    ipc: host # CRITICAL: Must match the agent
    restart: unless-stopped
    labels:
      - "com.centurylinklabs.watchtower.enable=false" # Critical service - manual updates only
    ports:
      - '80:80'   # HTTP
      - '443:443' # HTTPS
      - '81:81'   # Admin Web Port
    environment:
      - TZ=America/New_York
      - DISABLE_IPV6=true
    volumes:
      - ${NPM_DATA}:/data
      - ${NPM_LETSENCRYPT}:/etc/letsencrypt
      # Your custom config volume from previous setup
      - ./npm/custom-conf:/etc/nginx/custom
    networks:
      - npm_network
      - webproxy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:81/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # =======================================================
  # 2. YOUR CUSTOM SERVICES (Preserved)
  # =======================================================
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    restart: always
    user: "2000:990"
    ports:
      - '9443:9443'
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./portainer/data:/data
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "curl", "-f", "https://127.0.0.1:9443/api/status || exit 1"]
      interval: 45s
      timeout: 10s
      retries: 3
      start_period: 30s

  uptime-kuma:
    image: louislam/uptime-kuma:latest
    container_name: uptime-kuma
    restart: always
    environment:
      - TZ=America/New_York
      - KUMA_ENABLE_PROMETHEUS=true
    volumes:
      - ./uptime-kuma/data:/app/data
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "node", "extra/healthcheck.js"]
      interval: 30s
      timeout: 10s
      retries: 3

  filebrowser:
    image: filebrowser/filebrowser:latest
    container_name: filebrowser
    restart: always
    user: "1000:1000"
    environment:
      - PUID=1000
      - PGID=1000
    volumes:
      - ./filebrowser/config:/database
      - /home/jgoodloe/services:/srv/services-config
    networks:
      - webproxy
    command: -a 0.0.0.0 -p 80
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:80/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  homer:
    image: b4bz/homer:latest
    container_name: homer
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - ./homer/assets:/www/assets
    environment:
      - PUID=1000
      - PGID=1000
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8080/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  n8n:
    image: n8nio/n8n:latest
    container_name: n8n
    restart: unless-stopped
    ports:
      - "5678:5678"
    volumes:
      - ./n8n/data:/home/node/.n8n
    environment:
      - N8N_ENCRYPTION_KEY=j9YcK1rX4tY7mQ1rX4tY7cK3jV0nL5oA9uIeD8
      - NODE_ENV=production
      - GENERIC_TIMEZONE=America/New_York
      - N8N_HOST=n8n.goodloe.xyz
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_HASH=true
      - DB_SQLITE_POOL_SIZE=5
      - N8N_RUNNERS_ENABLED=true
      - N8N_BLOCK_ENV_ACCESS_IN_NODE=true
      - N8N_GIT_NODE_DISABLE_BARE_REPOS=true
      - WEBHOOK_TUNNEL_URL=https://n8n.goodloe.xyz/
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
    networks:
      - npm_network
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -f http://127.0.0.1:5678/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  wg-easy:
    image: ghcr.io/wg-easy/wg-easy:15
    container_name: wg-easy
    cap_add:
      - NET_ADMIN
    environment:
      - WEBSERVER_PORT=51821
      - WG_HOST=108.56.239.35
      - WG_PORT=51822
      - PASSWORD_HASH=$$2a$$10$$IfDzL6TL4.AjQWuDAq/8deqUJKQgewjJ8tZISMCG14TnUhRRHpOD.
      - ENABLE_PROMETHEUS_METRICS=true
    ports:
      - "51820:51820/udp"
      - "51821:51821/tcp"
    volumes:
      - ./wg-easy/data:/etc/wireguard
    sysctls:
      - net.ipv4.ip_forward=1
      - net.ipv4.conf.all.src_valid_mark=1
    restart: unless-stopped
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -f http://127.0.0.1:51821/ || exit 1"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s

  # Custom Build Required: ./ocsp/Dockerfile must exist
  ocsp:
    build: ./ocsp
    ports:
      - "8678:8678"
    container_name: ocsp-service
    environment:
      - CERTIFICATE_PATH=/app/piv.xcloud.authentx.com.prm.crt
      - SCHEDULE_INTERVAL=300
      - CERT_EXPIRY_WARNING_DAYS=30
      - CRL_EXPIRY_WARNING_HOURS=3
      - CRL_ONLY=true
      - CRL_URLS=http://crl.xca.xpki.com/CRLs/XTec_PIVI_CA1.crl,http://crl.xcatest2.xpki.com/CRLs/XTec_PIVI_Test2_CA_1.crl
      - CRL_NOTIFICATIONS={"http://crl.xca.xpki.com/CRLs/XTec_PIVI_CA1.crl":{"http_push_url":"http://uptime-kuma:3001/api/push/ynOyE22dMh?status=up&msg=OK"},"http://crl.xcatest2.xpki.com/CRLs/XTec_PIVI_Test2_CA_1.crl":{"http_push_url":"http://uptime-kuma:3001/api/push/9TwqAsFUR0?status=up&msg=OK"}}
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -f http://127.0.0.1:8678/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  watchtower:
    image: containrrr/watchtower:latest
    container_name: watchtower
    # Using string format to ensure proper parsing
    command: --interval=21600 --cleanup --rolling-restart --include-restarting --stop-timeout=30s --enable-lifecycle-hooks
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_POLL_INTERVAL=21600
      - DOCKER_API_VERSION=1.44
      # Uncomment and configure one of the following notification methods:
      # - WATCHTOWER_NOTIFICATION_SLACK_HOOK_URL=https://hooks.slack.com/services/xxx
      # - WATCHTOWER_NOTIFICATION_DISCORD_HOOK_URL=https://discord.com/api/webhooks/xxxx
      # - WATCHTOWER_NOTIFICATION_EMAIL_FROM=updates@yourdomain
      # - WATCHTOWER_NOTIFICATION_EMAIL_TO=you@yourdomain
    healthcheck:
      test: ["CMD", "bash", "-lc", "ps aux | grep watchtower | grep -v grep || exit 1"]
      interval: 5m
      timeout: 10s
      retries: 3

  code-server:
    image: lscr.io/linuxserver/code-server:latest
    container_name: code-server
    restart: unless-stopped
    user: "1000:1000"
    environment:
      - PUID=1000
      - PGID=1000
      - DOCKER_USER=jgoodloe
      - TZ=America/New_York
      - DOCKER_MODS=linuxserver/mods:code-server-vscodium
      - HASHED_PASSWORD=$$argon2i$$v=19$$m=4096,t=3,p=1$$d3N0NXFoYmdrMmx1MWloNGRtdXh2Zw$$L+YRY6evarNj3Hh1Uqh4iQ
      - PROXY_DOMAIN=code-server.goodloe.xyz
      - DEFAULT_WORKSPACE=/config/workspace
      - PWA_APPNAME=code-server
    volumes:
      - ./code-server/config:/config
      - /path/to/your/code/projects:/config/workspace
      - ./homer/assets/:/config/homer
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -f http://127.0.0.1:8080/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # -------------------------
  # MONITORING STACK
  # -------------------------
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    volumes:
      - ./grafana/data:/var/lib/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    user: "65534:65534"  # Run as nobody user to match Prometheus default
    labels:
      - "com.centurylinklabs.watchtower.enable=false" # Critical service - manual updates only
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--query.max-concurrency=20'
      - '--query.max-samples=50000000'
      # Disable query logging to avoid permission issues (optional - can be re-enabled after fixing permissions)
      - '--query.active-query-tracker-path=/prometheus/queries.active'
    volumes:
      # Requires ./prometheus/prometheus.yml to exist
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/data:/prometheus
    ports:
      - "9091:9090"
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:9090/-/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    restart: unless-stopped
    privileged: true
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    ports:
      - "8081:8080"
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8080/healthz || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  loki:
    image: grafana/loki:latest
    container_name: loki
    restart: unless-stopped
    labels:
      - "com.centurylinklabs.watchtower.enable=false" # Critical service - manual updates only
    volumes:
      # Requires ./loki/loki-config.yml to exist
      - ./loki/loki-config.yml:/etc/loki/config.yml
      - ./loki/data:/loki/data
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/config.yml
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -f http://127.0.0.1:3100/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  promtail:
    image: grafana/promtail:latest
    container_name: promtail
    restart: unless-stopped
    user: root
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # Requires ./loki/promtail-config.yml to exist
      - ./loki/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/log:/var/log:ro
    command: -config.expand-env=true -config.file=/etc/promtail/config.yml
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "ps aux | grep promtail | grep -v grep || exit 1"]
      interval: 1m
      timeout: 10s
      retries: 3

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    pid: host
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/host'
      - '--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/run/docker.sock)$'
      - --collector.cpu
      - --collector.meminfo
    ports:
      - "9100:9100"
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -f http://127.0.0.1:9100/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  blackbox-exporter:
    image: prom/blackbox-exporter:latest
    container_name: blackbox-exporter
    restart: unless-stopped
    volumes:
      # Requires ./prometheus/blackbox-config.yml to exist
      - ./prometheus/blackbox-config.yml:/config/blackbox.yml:ro
    command:
      - '--config.file=/config/blackbox.yml'
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -f http://127.0.0.1:9115/metrics || exit 1"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 10s

  nginx-exporter:
    image: nginx/nginx-prometheus-exporter:latest
    container_name: nginx-exporter
    restart: unless-stopped
    # Scrapes the new NPM container
    command:
      - '-nginx.scrape-uri=http://appsec-nginx-proxy-manager/stub_status'
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -f http://127.0.0.1:9113/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  fail2ban:
    image: crazymax/fail2ban:latest
    container_name: fail2ban
    cap_add:
      - NET_ADMIN
    restart: unless-stopped
    volumes:
      # Updated volume path to match new NPM container logs
      - ${NPM_DATA}/logs:/var/log/npm:ro
      - ./fail2ban/data:/data
      - /etc/localtime:/etc/localtime:ro
    environment:
      - TZ=America/New_York
      - F2B_IPTABLES_CHAIN=DOCKER-USER
    networks:
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "fail2ban-client ping || exit 1"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s

  crowdsec:
    image: crowdsecurity/crowdsec:v1.7.2
    container_name: crowdsec
    restart: unless-stopped
    labels:
      - "com.centurylinklabs.watchtower.enable=false" # Critical service - manual updates only
    cap_add:
      - NET_ADMIN
      - NET_RAW
    security_opt:
      - no-new-privileges:true
    volumes:
      # Updated volume path to match new NPM container logs
      - ${NPM_DATA}/logs:/var/log/nginx:ro
      - ./crowdsec/config:/etc/crowdsec
      - ./crowdsec/data:/var/lib/crowdsec/data
      - /etc/localtime:/etc/localtime:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      # Note: If automatic collection installation fails, remove COLLECTIONS line and install manually:
      # docker exec crowdsec cscli hub update
      # docker exec crowdsec cscli collections install crowdsecurity/nginx
      # Alternative: Update to latest CrowdSec version (v1.7.3) which may have better collection support
      - COLLECTIONS=crowdsecurity/nginx
      - TZ=America/New_York
    networks:
      - crowdsec_net
      - webproxy
    healthcheck:
      test: ["CMD", "bash", "-lc", "cscli metrics || exit 1"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s

  falco-driver-loader:
    image: falcosecurity/falco-driver-loader:latest
    container_name: falco-driver-loader
    privileged: true
    volumes:
      - /lib/modules:/lib/modules:ro
      - /usr/src:/usr/src:rw
      - /tmp/falco-driver:/opt/falco-driver:rw
    environment:
      - FALCO_DRIVER_NAME=falco
      - FALCO_TARGET=generic
      - FALCO_DRIVER_TYPE=modern_ebpf
    healthcheck:
      test: ["CMD", "bash", "-lc", "test -f /opt/falco-driver/falco.ko || exit 1"]
      interval: 2m
      timeout: 10s
      retries: 2
      start_period: 1m

  falco:
    image: falcosecurity/falco:latest
    container_name: falco
    restart: unless-stopped
    labels:
      - "com.centurylinklabs.watchtower.enable=false" # Critical service - manual updates only
    cap_add:
      - SYS_PTRACE
      - NET_ADMIN
      - IPC_LOCK
      - DAC_READ_SEARCH
    privileged: true
    volumes:
      - /proc:/host/proc:ro
      - /dev:/host/dev
      - /sys:/host/sys:ro
      - /etc:/host/etc:ro
      - /usr:/host/usr:ro
      - /lib/modules:/host/lib/modules:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./falco/config:/etc/falco/
      - /tmp/falco-driver:/opt/falco-driver:ro
    environment:
      - FALCO_BPF_ENABLED=false
      - KUBERNETES_ENABLED=false
      - FALCO_DRIVER_TYPE=kmod
    networks:
      - webproxy
    pid: host
    healthcheck:
      test: ["CMD", "bash", "-lc", "ps aux | grep falco | grep -v grep || exit 1"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 1m

networks:
  webproxy:
    driver: bridge
  npm_network:
    name: nginx_proxy_manager_network
    external: true
  crowdsec_net:
    driver: bridge
